# -*- coding: utf-8 -*-
"""R-06 âœ… 1. Multivariate (Single layer NN).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1j98IedHxEYm_ASaEtgWGoY1NpbtPhwWW

# Importing libraries
"""

from google.colab import drive
from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics import mean_absolute_error

import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
import tensorflow as tf

tf.config.run_functions_eagerly(True)

"""# Data loading"""

drive.mount('/content/drive')

national_df = pd.read_csv('/content/drive/MyDrive/pepper_mate_models/price_prediction_models/all_ml_models/national_df_multivariate_models/dataset/augmented_time_series.csv')

national_df.head()

national_df.info()

"""# Extract relevant features for the multivariate model"""

# Extract relevant features for the multivariate model
features = national_df[['gr1_high_price', 'gr1_avg_price', 'gr2_high_price', 'gr2_avg_price', 'white_high_price', 'white_avg_price']]

"""# Normalize the features"""

# Normalize the features
scaler = MinMaxScaler(feature_range=(0, 1))
normalized_features = scaler.fit_transform(features)

normalized_features

# Convert normalized data to DataFrame
normalized_features = pd.DataFrame(normalized_features, columns=features.columns)

normalized_features

"""# Training, validation split"""

# Split the data into training and validation sets
# 3165 * 0.8 = 2532
split_time = 2532
x_train = normalized_features[:split_time]
x_valid = normalized_features[split_time:]

x_train.head()

x_train.shape

"""# Function for windowing the dataset"""

def windowed_dataset_multivariate(features, window_size, batch_size, shuffle_buffer_size):
    """
    Creates a windowed dataset for time series forecasting.

    Args:
        features: The 6 input time series data.
        window_size: The size of the sliding window.
        batch_size: The batch size for training.
        shuffle_buffer_size: The buffer size for shuffling the data.

    Returns:
        A tf.data.Dataset object representing the windowed dataset.
    """
    dataset = tf.data.Dataset.from_tensor_slices(features)
    dataset = dataset.window(window_size + 1, shift=1, drop_remainder=True)
    dataset = dataset.flat_map(lambda window: window.batch(window_size + 1))
    dataset = dataset.shuffle(shuffle_buffer_size)
    dataset = dataset.map(lambda window: (window[:-1], window[-1]))
    dataset = dataset.batch(batch_size).prefetch(1)
    return dataset

"""# Calling the windowing function to create the dataset


"""

# Set parameters for windowing
window_size = 78
batch_size = 32
shuffle_buffer_size = 1000

# Create windowed training and validation datasets
windowed_training_dataset = windowed_dataset_multivariate(x_train.values, window_size, batch_size, shuffle_buffer_size)
windowed_validation_dataset = windowed_dataset_multivariate(x_valid.values, window_size, batch_size, shuffle_buffer_size)

windowed_training_dataset

windowed_validation_dataset

"""# Model building"""

# Build the model for multivariate input
model = tf.keras.models.Sequential([
    tf.keras.Input(shape=(window_size, len(features.columns))),  # Input shape with 'window_size' and '6' features
    tf.keras.layers.Flatten(),  # Flatten the input for the dense layer
    tf.keras.layers.Dense(100, activation='relu'),
    tf.keras.layers.Dense(1)  # Output layer
])

model.summary()

"""# Model compiling"""

# Define an exponential decay schedule
initial_learning_rate = 1e-4
lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(
    initial_learning_rate=initial_learning_rate,
    decay_steps=1000,
    decay_rate=0.9,
    staircase=True
)

# Compile the model
model.compile(
    loss='mse',
    optimizer=tf.keras.optimizers.SGD(learning_rate=lr_schedule, momentum=0.9),
    metrics=['mae']  # Add accuracy metrics
)

"""# Model training"""

# Train the model
history = model.fit(windowed_training_dataset, epochs=500, validation_data=windowed_validation_dataset)

"""# Ploting the training loss and the validation loss


"""

import matplotlib.pyplot as plt

# Plot training and validation loss and accuracy as subplots
fig, axs = plt.subplots(1, 2, figsize=(10, 5))

# Plot training and validation loss
axs[0].plot(history.history['loss'], label='Train Loss')
axs[0].plot(history.history['val_loss'], label='Validation Loss')
axs[0].set_title('Model Loss')
axs[0].set_xlabel('Epochs')
axs[0].set_ylabel('Loss')
axs[0].legend()

# Plot training and validation MAE
axs[1].plot(history.history['mae'], label='Train MAE')
axs[1].plot(history.history['val_mae'], label='Validation MAE')
axs[1].set_title('Model MAE')
axs[1].set_xlabel('Epochs')
axs[1].set_ylabel('MAE')
axs[1].legend()

# Adjust layout for better spacing
plt.tight_layout()
plt.show()

"""# Forecasting the whole seriesn with the trained model


"""

# Forecasting the whole series
forecast = []
for time in range(len(normalized_features) - window_size):
    window_data = normalized_features.iloc[time:time + window_size].values
    forecast.append(model.predict(window_data[np.newaxis]))

forecast = forecast[split_time - window_size:]
results = np.array(forecast).squeeze()

"""# Plotting actual vs forecast"""

plt.figure(figsize=(20, 6))
plt.plot(x_valid.index, x_valid['gr1_high_price'], label='Actual')  # Plot one of the actual features
plt.plot(x_valid.index, results, label='Forecast')
plt.xlabel('Time')
plt.ylabel('gr1_high_price')
plt.title('Actual vs. Forecast gr1_high_price')
plt.legend()
plt.show()

# Plotting actual vs forecast for all features
fig, axes = plt.subplots(3, 2, figsize=(20, 18))
features_list = ['gr1_high_price', 'gr1_avg_price', 'gr2_high_price', 'gr2_avg_price', 'white_high_price', 'white_avg_price']

for i, feature in enumerate(features_list):
    ax = axes[i // 2, i % 2]
    ax.plot(x_valid.index, x_valid[feature], label='Actual')
    ax.plot(x_valid.index, results, label='Forecast')
    ax.set_xlabel('Time')
    ax.set_ylabel(feature)
    ax.set_title(f'Actual vs. Forecast {feature}')
    ax.legend()

plt.tight_layout()
plt.show()

"""# Calculating the mean absolute error"""

# Calculate mean absolute error
results_df = pd.DataFrame(results, index=x_valid.index, columns=['Forecast'])
train_mae = mean_absolute_error(x_valid['gr1_high_price'], results_df['Forecast'])
print(f"Mean Absolute Error: {train_mae}")

# Calculate mean absolute error for each feature
for feature in features_list:
    mae = mean_absolute_error(x_valid[feature], results)
    print(f"Mean Absolute Error for {feature}: {mae}")

"""# Calculating the loss and accuracy"""

# Extract the final metrics from the history object
final_loss = history.history['loss'][-1]
# final_val_loss = history.history['val_loss'][-1]
final_accuracy = history.history['mae'][-1]
# final_val_accuracy = history.history['val_mae'][-1]

# Print the final metrics
print(f"Final Training Loss: {final_loss:.4f}")
# print(f"Final Validation Loss: {final_val_loss:.4f}")
print(f"Final Training Accuracy (MAE): {final_accuracy:.4f}")
# print(f"Final Validation Accuracy (MAE): {final_val_accuracy:.4f}")

from sklearn.metrics import mean_absolute_error

# Assuming `results` is the model's predictions and `x_valid` contains the actual values
# Select one target feature for MAE and Accuracy calculation
target_feature = 'gr1_high_price'

# Get the actual values corresponding to the predictions
# Assuming 'results' corresponds to the last 61 samples of x_valid
actual_values = x_valid[target_feature][-len(results):]  # Select the last 61 samples

# Calculate MAE
mae = mean_absolute_error(actual_values, results)
print(f"Mean Absolute Error (MAE) for {target_feature}: {mae}")

# Calculate the Mean of Actual Values
mean_actual = actual_values.mean() # Calculate mean for the selected samples
print(f"Mean of Actual Values for {target_feature}: {mean_actual}")

# Calculate Accuracy
accuracy = 1 - (mae / mean_actual)
print(f"Accuracy for {target_feature}: {accuracy:.2%}")

"""# Saving the model"""

model.save('/content/drive/MyDrive/pepper_mate_models/price_prediction_models/all_ml_models/national_df_multivariate_models/saved_models/r_06_single_layer_nn.keras')

model.save('/content/drive/MyDrive/pepper_mate_models/price_prediction_models/all_ml_models/national_df_multivariate_models/saved_models/r_06_single_layer_nn.h5')