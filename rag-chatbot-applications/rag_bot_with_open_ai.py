# -*- coding: utf-8 -*-
"""✅ 3 rag_bot_with_open_ai.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1DX7-QbVHah7s-aFm2YdBSdeY75QmFEbn
"""

# Install necessary packages
!pip install langchain -qU
!pip install openai -qU
!pip install langchain-chroma -qU
!pip install langchain_community -qU
!pip install pypdf -qU
!pip install tiktoken -qU

import os
import tempfile
from langchain.chat_models import ChatOpenAI
from langchain.embeddings.openai import OpenAIEmbeddings
from langchain.document_loaders import PyPDFLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.vectorstores import Chroma
from langchain.prompts.chat import ChatPromptTemplate, MessagesPlaceholder
from langchain.chains.combine_documents import create_stuff_documents_chain
from langchain.chains.retrieval_qa.base import RetrievalQA
from langchain.memory import ChatMessageHistory

# Initialize OpenAI API Key
os.environ['OPENAI_API_KEY'] = 'your_openai_api_key'

# Initialize OpenAI LLM
llm = ChatOpenAI(
    model="gpt-4",
    temperature=0
)

# Initialize OpenAI Embedding Model
embedding_model = OpenAIEmbeddings(
    openai_api_key=os.environ['OPENAI_API_KEY']
)

# Create a temporary directory for Chroma
persist_directory = tempfile.mkdtemp()

# Load PDF Document
loader = PyPDFLoader("pepper_final.pdf")
try:
    docs = loader.load()
except Exception as e:
    print(f"Error loading PDF: {e}")
    docs = []

# Split Documents into Chunks
text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=400,
    chunk_overlap=50
)
splits = text_splitter.split_documents(docs)

# Create Vector Store and Retriever with temporary directory
vectorstore = Chroma.from_documents(
    documents=splits,
    embedding=embedding_model,
    persist_directory=persist_directory
)
retriever = vectorstore.as_retriever()

# Define Prompt Template
system_prompt = """You are an intelligent chatbot. Use the following context to answer the question.
If you don't know the answer, just say that you don't know.

{context}"""

prompt = ChatPromptTemplate.from_messages([
    ("system", system_prompt),
    ("human", "{input}"),
])

# Create RetrievalQA Chain
qa_chain = create_stuff_documents_chain(llm, prompt)
rag_chain = RetrievalQA.from_chain_type(
    llm=llm,
    chain_type="stuff",
    retriever=retriever,
    return_source_documents=False
)

# Manage Chat Session History
store = {}

def get_session_history(session_id: str) -> ChatMessageHistory:
    if session_id not in store:
        store[session_id] = ChatMessageHistory()
    return store[session_id]

# Conversational RAG Chain Logic
def conversational_rag_chain(input_text: str, session_id: str):
    # Retrieve session history
    history = get_session_history(session_id)
    # Append user input to history
    history.add_user_message(input_text)
    # Invoke RAG chain
    response = rag_chain.run({
        "query": input_text
    })
    # Append AI response to history
    history.add_ai_message(response)
    return response

# Example Usage
try:
    print("Using OpenAI Embedding Model:", embedding_model.model)
    response = conversational_rag_chain(
        input_text="ක්ෂණික මැලවීම රෝගය පාලනය කරගන්නෙ කොහොමද?",
        session_id="101"
    )
    print(response)
finally:
    # Clean up the temporary directory
    vectorstore.delete_collection()
    import shutil
    shutil.rmtree(persist_directory)

"""ක්ෂණික මැලවීම රෝගය පාලනය කරගන්නේ පහත පරිදිය:

1) ක්ෂේත්‍රයේ හාදින් ජලය බැසීමට සැලැස්වීම, නිදොස් ලෙස සවන පාලනය.
2) වැසි කාලයේදී වැල් වටා ඇති වසුන් ඉවත් කිරීම.
3) පාලව මට්ටම් සිට අඩි 1 1/2 ක් පමණ උසට ඇති හරස් අතු ඉවත් කිරීම.
4) ආසාදනය වූ අතු හා මුල් කොටස් ඉවත් කර පුළුස්සා දැමීම.
5) රෝගය දුටු මුල් අවස්ථාවේම ආසාදිත මුල් හාදින් පිරිසිදු කර මූණුතහි බෝඩෝ පාප්පය ආලේප කිරීම.





"""