# -*- coding: utf-8 -*-
"""✅ 2. rag_bot_with_llama_ai.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/18eLpO6h_Yw6p9s4n7buo1sqnP6Ycg8EJ
"""

# Install Necessary Packages
!pip install torch transformers langchain langchain-community langchain-chroma sentence-transformers pypdf -q

# Step 1: Authenticate with Hugging Face
from huggingface_hub import login

# Replace with your Hugging Face API token
login("your_huggingfacehub_api_token")

# Step 2: Initialize the Llama Model
from langchain_community.llms import HuggingFaceHub

# Initialize the HuggingFace Llama model
llm = HuggingFaceHub(
    repo_id="meta-llama/Llama-3.2-1b",  # Replace with the latest Llama model repo ID
    model_kwargs={"temperature": 0.1, "max_length": 500},
    huggingfacehub_api_token="your_huggingfacehub_api_token"
)

# Step 3: Initialize the Embedding Model
from langchain.embeddings import HuggingFaceEmbeddings

embedding_model = HuggingFaceEmbeddings(
    model_name="sentence-transformers/all-mpnet-base-v2"
)

# Step 4: Load PDF Document
from langchain_community.document_loaders import PyPDFLoader

# Replace with the path to your PDF file
loader = PyPDFLoader("pepper_final.pdf")
docs = loader.load()

# Step 5: Split Documents into Chunks
from langchain.text_splitter import RecursiveCharacterTextSplitter

# Initialize text splitter
text_splitter = RecursiveCharacterTextSplitter(chunk_size=400, chunk_overlap=50)
splits = text_splitter.split_documents(docs)

# Step 6: Create Vector Store and Retriever
from langchain.vectorstores import Chroma

# Create vector store and retriever
vectorstore = Chroma.from_documents(documents=splits, embedding=embedding_model)
retriever = vectorstore.as_retriever()

# Step 7: Define Prompt Template
from langchain.prompts import PromptTemplate

# Define the prompt template
template = """
Answer the question using the provided context only. If the context does not contain enough information, reply "I am unable to answer this question based on the provided context."

Question: {question}

Context:
{context}

Answer:
"""
prompt = PromptTemplate.from_template(template)

# Step 8: Create Conversational Retrieval Chain
from langchain.chains import ConversationalRetrievalChain

# Create the chain
chain = ConversationalRetrievalChain.from_llm(
    llm=llm,
    retriever=retriever,
    return_source_documents=True,  # Optional: Return source documents with the response
)

# Step 9: Invoke the Chain with an Example Question
chat_history = []  # Maintain chat history for conversational retrieval
question = "ගම්මිරිස් වලට වැලදිය හැකි රෝග මොනවාද?"
response = chain({"question": question, "chat_history": chat_history})

# Print the response
print(response["answer"])

# If you want to maintain chat history for a conversational session:
chat_history.append((question, response["answer"]))