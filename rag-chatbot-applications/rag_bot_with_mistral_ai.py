# -*- coding: utf-8 -*-
"""✅ 1. rag_bot_with_mistral_ai.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1_qQRcojc2aD3J6mx940sPdmXg5ntXPm1

# RAG Application with LangChain and HuggingFace
"""

# Install the necessary packages
!pip install torch -q
!pip install transformers -q
!pip install numpy -q
!pip install langchain -q
!pip install langchain_community -q
!pip install langchain-chroma -q
!pip install sentence_transformers -q

import os
from google.colab import userdata

"""# Initialize HuggingFace LLM

Model repo url: https://huggingface.co/mistralai/Mistral-7B-v0.1
"""

from huggingface_hub import login
login("your_huggingfacehub_api_token")

from langchain_community.llms import HuggingFaceHub

# Initialize the HuggingFace llm
llm = HuggingFaceHub(
    repo_id="mistralai/Mistral-7B-v0.1",
    model_kwargs={"temperature": 0.1, "max_length": 500},
    huggingfacehub_api_token="your_huggingfacehub_api_token"  # Insert your token here
)

"""# Initialize Embedding Model

Model url: https://sbert.net/
"""

from langchain.embeddings import HuggingFaceEmbeddings

embedding_model = HuggingFaceEmbeddings(
  model_name="sentence-transformers/all-mpnet-base-v2"
)

"""# Initialize Output Parser"""

from langchain.schema.output_parser import StrOutputParser

output_parser=StrOutputParser()

"""# Load PDF Document"""

!pip install pypdf -qU

from langchain_community.document_loaders import PyPDFLoader

# Load the PDF document
loader = PyPDFLoader("/content/pepper_final.pdf")

docs = loader.load()

len(docs)

docs[0]

"""# Split Documents into Chunks"""

from langchain_text_splitters import RecursiveCharacterTextSplitter

# Initialize the text splitter
text_splitter = RecursiveCharacterTextSplitter(chunk_size=400, chunk_overlap=50)

# Split the documents into chunks
splits = text_splitter.split_documents(docs)

len(splits)

"""# Create Vector Store and Retriever"""

from langchain_chroma import Chroma

# Create a vector store from the document chunks
vectorstore = Chroma.from_documents(documents=splits, embedding=embedding_model)

# Create a retriever from the vector store
retriever = vectorstore.as_retriever()

"""# Define Prompt Template"""

from langchain.prompts import ChatPromptTemplate

# Define prompt template
template = """
Answer this question using the provided context only. If the context does not contain enough information to answer, say "I am unable to answer this question based on the provided context.". Do not use any external knowledge or previous interactions.

   {question}

   Context:
   {context}

   Answer:
   """

prompt=ChatPromptTemplate.from_template(template)

prompt

"""# Chain Retriever and Prompt Template with LLM"""

from langchain.schema.runnable import RunnablePassthrough

chain = (
    {"context": retriever,  "question": RunnablePassthrough()}
    | prompt
    | llm
    | output_parser
)

"""# Invoke RAG Chain with Example Questions"""

response = chain.invoke("ගම්මිරිස් වලට වැලදිය හැකි රෝග මොනවාද?")
print(response)

# Step 1: Print document content to confirm loading
print("Document Content:", docs[0].page_content[:500])  # Print the first 500 characters

# Step 2: Check chunks created
print("Number of Chunks:", len(splits))
for i, chunk in enumerate(splits):
    print(f"Chunk {i}:", chunk.page_content[:500])  # Print each chunk

# Step 3: Test retrieval
sample_question = "ගම්මිරිස් වලට වැලදිය හැකි රෝග මොනවාද?"
retrieved_docs = retriever.get_relevant_documents(sample_question)
print("Retrieved Docs:", [doc.page_content[:500] for doc in retrieved_docs])

# Step 4: Validate prompt
context = " ".join([doc.page_content for doc in retrieved_docs])
print("Generated Prompt Context:", context)
print("Prompt Input:", prompt.format(context=context, question=sample_question))

# Step 5: Test chain execution
response = chain.invoke(sample_question)
print("Chain Response:", response)

# Directly query the retriever
question = "ගම්මිරිස් වලට වැලදිය හැකි රෝග මොනවාද?"
retrieved_docs = retriever.get_relevant_documents(question)

# Check the results
for i, doc in enumerate(retrieved_docs):
    print(f"Document {i}: {doc.page_content}")