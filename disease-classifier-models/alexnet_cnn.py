# -*- coding: utf-8 -*-
"""âœ… 2. AlexNet CNN.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1L4xThcvNOd9IpyvxEDk7b2Utde2oUwwn

# Imports
"""

# Importing essential packages
import os
import matplotlib.pyplot as plt
import tensorflow as tf

"""# Data Loading"""

# Mounting google drive
from google.colab import drive
drive.mount('/content/drive')

# Specifying file paths for the data directories
TRAIN_DIR = '/content/drive/MyDrive/pepper_mate_models/disease_classification_models/dataset/train/'
VALIDATION_DIR = '/content/drive/MyDrive/pepper_mate_models/disease_classification_models/dataset/valid/'
TEST_DIR = '/content/drive/MyDrive/pepper_mate_models/disease_classification_models/dataset/test/'

# Specifying file paths for the each class in the training dataset
pollu_beetle_dir = os.path.join(TRAIN_DIR, 'pollu_beetle')
brown_spot_dir = os.path.join(TRAIN_DIR, 'brown_spot')
healthy_dir = os.path.join(TRAIN_DIR, 'healthy')
leaf_blight_dir = os.path.join(TRAIN_DIR, 'leaf_blight')
little_leaf_dir = os.path.join(TRAIN_DIR, 'little_leaf')
quick_wilt_dir = os.path.join(TRAIN_DIR, 'quick_wilt')
slow_wilt_dir = os.path.join(TRAIN_DIR, 'slow_wilt')
white_spot_dir = os.path.join(TRAIN_DIR, 'white_spot')
thrips_dir = os.path.join(TRAIN_DIR, 'thrips')

"""# Data visualization"""

# Loading first example of the each class
sample_pollu_beetle = tf.keras.preprocessing.image.load_img(os.path.join(pollu_beetle_dir, os.listdir(pollu_beetle_dir)[0]))
sample_brown_spot = tf.keras.preprocessing.image.load_img(os.path.join(brown_spot_dir, os.listdir(brown_spot_dir)[0]))
sample_healthy = tf.keras.preprocessing.image.load_img(os.path.join(healthy_dir, os.listdir(healthy_dir)[0]))
sample_leaf_blight = tf.keras.preprocessing.image.load_img(os.path.join(leaf_blight_dir, os.listdir(leaf_blight_dir)[0]))
sample_little_leaf_disease = tf.keras.preprocessing.image.load_img(os.path.join(little_leaf_dir, os.listdir(little_leaf_dir)[0]))
sample_quick_wilt = tf.keras.preprocessing.image.load_img(os.path.join(quick_wilt_dir, os.listdir(quick_wilt_dir)[0]))
sample_slow_wilt = tf.keras.preprocessing.image.load_img(os.path.join(slow_wilt_dir, os.listdir(slow_wilt_dir)[0]))
sample_white_spot = tf.keras.preprocessing.image.load_img(os.path.join(white_spot_dir, os.listdir(white_spot_dir)[0]))
sample_thrips = tf.keras.preprocessing.image.load_img(os.path.join(thrips_dir, os.listdir(thrips_dir)[0]))

# Create a figure and an array of subplots
fig, axes = plt.subplots(3, 3, figsize=(10, 10))

# Flatten the axes array for easier iteration
axes = axes.ravel()

# Define a list of sample images and their corresponding titles
sample_images = [
    sample_pollu_beetle, sample_brown_spot, sample_healthy,
    sample_leaf_blight, sample_little_leaf_disease, sample_quick_wilt,
    sample_slow_wilt, sample_white_spot, sample_thrips
]
titles = [
    'Pollu Beetle', 'Brown Spot', 'Healthy',
    'Leaf Blight', 'Little Leaf', 'Quick Wilt',
    'Slow Wilt', 'White Spot', 'Thrips'
]

# Iterate through the sample images and display them in the subplots
for i in range(9):  # Assuming you have 9 sample images
    axes[i].imshow(sample_images[i])
    axes[i].set_title(titles[i])
    axes[i].axis('off')

# Adjust the spacing between subplots and display the plot
plt.tight_layout()
plt.show()

# Converting an image into its numpy array representation
sample_array = tf.keras.preprocessing.image.img_to_array(sample_healthy)

print(f"Each image has shape: {sample_array.shape}")

"""# Creating training, validation and test sets"""

# Defining a function to create the training, validation and test sets
def train_val_test_datasets():
    """Creates training, validation and test datasets

    Returns:
        (tf.data.Dataset, tf.data.Dataset): training, validation and test datasets
    """

    # Create the training dataset
    # Here we have used image generators (it will do image labeling for us)
    training_dataset = tf.keras.utils.image_dataset_from_directory(
        directory=TRAIN_DIR,        # Use the training directory
        batch_size=32,              # Set batch size to 32
        image_size=(640, 640),      # Resize images to 500x500
        shuffle=True,               # Shuffle the dataset
        seed=7,                     # Set seed for reproducibility
        label_mode='categorical'    # Use categorical labels for multi-class classification
    )

    # Create the validation dataset
    validation_dataset = tf.keras.utils.image_dataset_from_directory(
        directory=VALIDATION_DIR,   # Use the validation directory
        batch_size=32,              # Set batch size to 32
        image_size=(640, 640),      # Resize images to 500x500
        shuffle=True,               # Shuffle the dataset
        seed=7,                     # Set seed for reproducibility
        label_mode='categorical'    # Use categorical labels for multi-class classification
    )

    # Create the test dataset
    test_dataset = tf.keras.utils.image_dataset_from_directory(
        directory=TEST_DIR,         # Use the test directory
        batch_size=32,              # Set batch size to 32
        image_size=(640, 640),      # Resize images to 500x500
        shuffle=True,               # Shuffle the dataset
        seed=7,                     # Set seed for reproducibility
        label_mode='categorical'    # Use categorical labels for multi-class classification
    )

    return training_dataset, validation_dataset, test_dataset

# Calling the dataset generation function
training_dataset, validation_dataset, test_dataset = train_val_test_datasets()

# Displaying a labels Tensor
for images, labels in training_dataset.take(1):
  print(labels)

# Counting number of batches in each dataset
print(f'Number of training batches: {training_dataset.cardinality()}')
print(f'Number of validation batches: {validation_dataset.cardinality()}')
print(f'Number of test batches: {test_dataset.cardinality()}')

"""# Modeling"""

import tensorflow as tf

# Defining a function to create the model
def create_model():
    """Create the classifier model

    Returns:
        tf.keras.Model: CNN for multi-class classification
    """
    # Define the model
    model = tf.keras.models.Sequential([
        tf.keras.layers.Conv2D(96, 11, strides=(4, 4), padding='valid', input_shape=(640, 640, 3), activation='relu'),
        tf.keras.layers.MaxPooling2D(pool_size=(2, 2), strides=(2, 2), padding='valid'),
        tf.keras.layers.BatchNormalization(),
        tf.keras.layers.Conv2D(256, 11, strides=(1, 1), padding='valid', activation='relu'),
        tf.keras.layers.MaxPooling2D(pool_size=(2, 2), strides=(2, 2), padding='valid'),
        tf.keras.layers.BatchNormalization(),
        tf.keras.layers.Conv2D(384, 3, strides=(1, 1), padding='valid', activation='relu'),
        tf.keras.layers.BatchNormalization(),
        tf.keras.layers.Conv2D(384, 3, strides=(1, 1), padding='valid', activation='relu'),
        tf.keras.layers.BatchNormalization(),
        tf.keras.layers.Conv2D(256, 3, strides=(1, 1), padding='valid', activation='relu'),
        tf.keras.layers.MaxPooling2D(pool_size=(2, 2), strides=(2, 2), padding='valid'),
        tf.keras.layers.BatchNormalization(),
        tf.keras.layers.Flatten(),
        tf.keras.layers.Dense(4096, activation='relu'),
        tf.keras.layers.Dropout(0.4),
        tf.keras.layers.BatchNormalization(),
        tf.keras.layers.Dense(4096, activation='relu'),
        tf.keras.layers.Dropout(0.4),
        tf.keras.layers.BatchNormalization(),
        tf.keras.layers.Dense(1000, activation='relu'),
        tf.keras.layers.Dropout(0.2),
        tf.keras.layers.BatchNormalization(),
        tf.keras.layers.Dense(9, activation='softmax')
    ])

    # Use ExponentialDecay to adjust the learning rate over time
    lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(
        initial_learning_rate=0.001,
        decay_steps=10000,
        decay_rate=0.96,
        staircase=True
    )

    # Compile the model
    model.compile(optimizer=tf.keras.optimizers.SGD(learning_rate=lr_schedule, momentum=0.9),
                  loss='categorical_crossentropy',
                  metrics=['accuracy'])

    return model

# Calling the model creation function
model = create_model()

"""# Check the model architecture"""

# Checking that the input and output shape of your model are correct
print(f'Input shape: {model.input_shape}')
print(f'Output shape: {model.output_shape}')

# Checking the model summary
model.summary()

"""# Load alexnet best weights"""

# Load weights by name and skip mismatches
model.load_weights('/content/drive/MyDrive/pepper_mate_models/disease_classification_models/alexnet_best_weights_9.hdf5', by_name=True, skip_mismatch=True)

"""# Fine Tuning By Freezing Some Layers Of Our Model"""

# let's visualize layer names and layer indices to see how many layers
# we should freeze:
from keras import layers
for i, layer in enumerate(model.layers):
   print(i, layer.name)

# we chose to train the top 2 conv blocks, i.e. we will freeze
# the first 8 layers and unfreeze the rest:
print("Freezed layers:")
for i, layer in enumerate(model.layers[:20]):
    print(i, layer.name)
    layer.trainable = False

"""# Model summary after freezing"""

#trainable parameters decrease after freezing some bottom layers
model.summary()

"""# Checking the model architecture compatibilty with the dataset"""

# Check that the architecture you used is compatible with the dataset (you can ignore the warnings prompted by using the GPU):
for images, labels in training_dataset.take(1):
	example_batch_images = images
	example_batch_labels = labels

try:
	model.evaluate(example_batch_images, example_batch_labels, verbose=False)
except:
	print("Your model is not compatible with the dataset you defined earlier. Check that the loss function, last layer and label_mode are compatible with one another.")
else:
	predictions = model.predict(example_batch_images, verbose=False)
	print(f"predictions have shape: {predictions.shape}")

"""# Model training"""

# Training the model with early stopping
history = model.fit(
    training_dataset,
    epochs=25,
    validation_data=validation_dataset,
    # callbacks=[early_stopping]
)

"""# Model evaluation"""

# Get training and validation accuracies
acc = history.history['accuracy']
val_acc = history.history['val_accuracy']
loss = history.history['loss']
val_loss = history.history['val_loss']

# Get number of epochs
epochs = range(len(acc))

# Creating subplots for accuracy and loss
fig, ax = plt.subplots(1, 2, figsize=(10, 5))
fig.suptitle('Training and validation accuracy and loss')

# Plotting accuracy
ax[0].plot(epochs, acc, 'r', label='Training accuracy')
ax[0].plot(epochs, val_acc, 'b', label='Validation accuracy')
ax[0].set_title('Training and validation accuracy')
ax[0].legend()
ax[0].set_xlabel('Epochs')
ax[0].set_ylabel('Accuracy')

# Plotting loss
ax[1].plot(epochs, loss, 'r', label='Training loss')
ax[1].plot(epochs, val_loss, 'b', label='Validation loss')
ax[1].set_title('Training and validation loss')
ax[1].legend()
ax[1].set_xlabel('Epochs')
ax[1].set_ylabel('Loss')

plt.show()

# calculating the testung accuracy and the testing loss
test_loss, test_accuracy = model.evaluate(test_dataset)
print(f'Test loss: {test_loss},\nTest accuracy: {test_accuracy}')

"""# Saving the model"""

# Saving model to the google drive
model.save('/content/drive/MyDrive/pepper_mate_models/disease_classification_models/saved_models/alexnet_cnn.keras')
model.save('/content/drive/MyDrive/pepper_mate_models/disease_classification_models/saved_models/alexnet_cnn.h5')

# prompt: calculate training validation and test accucaries and loss

# Training, validation, and test metrics are already calculated and printed in the provided code.
# Here's a slightly reorganized version to make it more explicit:

# ... (Your existing code) ...

# Model evaluation
# Get training and validation accuracies
acc = history.history['accuracy']
val_acc = history.history['val_accuracy']
loss = history.history['loss']
val_loss = history.history['val_loss']

# Get number of epochs
epochs = range(len(acc))


# ... (Plotting code) ...

# Calculate and print test accuracy and loss
test_loss, test_accuracy = model.evaluate(test_dataset)
print(f'Test Loss: {test_loss}')
print(f'Test Accuracy: {test_accuracy}')


# Store the metrics in variables for easier access
training_accuracy = acc[-1]  # Accuracy from the last training epoch
validation_accuracy = val_acc[-1] # Accuracy from the last validation epoch
training_loss = loss[-1] # Loss from the last training epoch
validation_loss = val_loss[-1] # Loss from the last validation epoch

print("\nSummary of Metrics:")
print(f"Training Accuracy: {training_accuracy}")
print(f"Validation Accuracy: {validation_accuracy}")
print(f"Training Loss: {training_loss}")
print(f"Validation Loss: {validation_loss}")
print(f"Test Loss: {test_loss}")
print(f"Test Accuracy: {test_accuracy}")

# ... (Rest of your code) ...