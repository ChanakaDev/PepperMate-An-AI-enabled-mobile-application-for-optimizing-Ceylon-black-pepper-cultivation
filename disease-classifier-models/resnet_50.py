# -*- coding: utf-8 -*-
"""2nd round âœ… 5. ResNet_50.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1p8efi1mJDtIw_UuDGrHGWmPCGoEH11OA

# Imports
"""

import os
import tensorflow as tf
import matplotlib.pyplot as plt

"""# Data Loading"""

from google.colab import drive
drive.mount('/content/drive')

# Specifying file paths for the data directories
TRAIN_DIR = '/content/drive/MyDrive/pepper_mate_models/disease_classification_models/dataset/train/'
VALIDATION_DIR = '/content/drive/MyDrive/pepper_mate_models/disease_classification_models/dataset/valid/'
TEST_DIR = '/content/drive/MyDrive/pepper_mate_models/disease_classification_models/dataset/test/'

# Specifying file paths for the each class in the training dataset
pollu_beetle_dir = os.path.join(TRAIN_DIR, 'pollu_beetle')
brown_spot_dir = os.path.join(TRAIN_DIR, 'brown_spot')
healthy_dir = os.path.join(TRAIN_DIR, 'healthy')
leaf_blight_dir = os.path.join(TRAIN_DIR, 'leaf_blight')
little_leaf_dir = os.path.join(TRAIN_DIR, 'little_leaf')
quick_wilt_dir = os.path.join(TRAIN_DIR, 'quick_wilt')
slow_wilt_dir = os.path.join(TRAIN_DIR, 'slow_wilt')
white_spot_dir = os.path.join(TRAIN_DIR, 'white_spot')
thrips_dir = os.path.join(TRAIN_DIR, 'thrips')

"""# Data visualization"""

# Loading first example of the each class
sample_pollu_beetle = tf.keras.preprocessing.image.load_img(os.path.join(pollu_beetle_dir, os.listdir(pollu_beetle_dir)[0]))
sample_brown_spot = tf.keras.preprocessing.image.load_img(os.path.join(brown_spot_dir, os.listdir(brown_spot_dir)[0]))
sample_healthy = tf.keras.preprocessing.image.load_img(os.path.join(healthy_dir, os.listdir(healthy_dir)[0]))
sample_leaf_blight = tf.keras.preprocessing.image.load_img(os.path.join(leaf_blight_dir, os.listdir(leaf_blight_dir)[0]))
sample_little_leaf_disease = tf.keras.preprocessing.image.load_img(os.path.join(little_leaf_dir, os.listdir(little_leaf_dir)[0]))
sample_quick_wilt = tf.keras.preprocessing.image.load_img(os.path.join(quick_wilt_dir, os.listdir(quick_wilt_dir)[0]))
sample_slow_wilt = tf.keras.preprocessing.image.load_img(os.path.join(slow_wilt_dir, os.listdir(slow_wilt_dir)[0]))
sample_white_spot = tf.keras.preprocessing.image.load_img(os.path.join(white_spot_dir, os.listdir(white_spot_dir)[0]))
sample_thrips = tf.keras.preprocessing.image.load_img(os.path.join(thrips_dir, os.listdir(thrips_dir)[0]))

# Create a figure and an array of subplots
fig, axes = plt.subplots(3, 3, figsize=(10, 10))

# Flatten the axes array for easier iteration
axes = axes.ravel()

# Define a list of sample images and their corresponding titles
sample_images = [
    sample_pollu_beetle, sample_brown_spot, sample_healthy,
    sample_leaf_blight, sample_little_leaf_disease, sample_quick_wilt,
    sample_slow_wilt, sample_white_spot, sample_thrips
]
titles = [
    'Pollu Beetle', 'Brown Spot', 'Healthy',
    'Leaf Blight', 'Little Leaf', 'Quick Wilt',
    'Slow Wilt', 'White Spot', 'Thrips'
]

# Iterate through the sample images and display them in the subplots
for i in range(9):  # Assuming you have 9 sample images
    axes[i].imshow(sample_images[i])
    axes[i].set_title(titles[i])
    axes[i].axis('off')

# Adjust the spacing between subplots and display the plot
plt.tight_layout()
plt.show()

# Converting an image into its numpy array representation
sample_array = tf.keras.preprocessing.image.img_to_array(sample_healthy)

print(f"Each image has shape: {sample_array.shape}")

"""# Creating training, validation and test sets"""

# Defining a function to create the training, validation and test sets
def train_val_test_datasets():
    """Creates training, validation and test datasets

    Returns:
        (tf.data.Dataset, tf.data.Dataset): training, validation and test datasets
    """

    # Create the training dataset
    # Here we have used image generators (it will do image labeling for us)
    training_dataset = tf.keras.utils.image_dataset_from_directory(
        directory=TRAIN_DIR,        # Use the training directory
        batch_size=32,              # Set batch size to 32
        image_size=(640, 640),      # Resize images to 500x500
        shuffle=True,               # Shuffle the dataset
        seed=7,                     # Set seed for reproducibility
        label_mode='categorical'    # Use categorical labels for multi-class classification
    )

    # Create the validation dataset
    validation_dataset = tf.keras.utils.image_dataset_from_directory(
        directory=VALIDATION_DIR,   # Use the validation directory
        batch_size=32,              # Set batch size to 32
        image_size=(640, 640),      # Resize images to 500x500
        shuffle=True,               # Shuffle the dataset
        seed=7,                     # Set seed for reproducibility
        label_mode='categorical'    # Use categorical labels for multi-class classification
    )

    # Create the test dataset
    test_dataset = tf.keras.utils.image_dataset_from_directory(
        directory=TEST_DIR,         # Use the test directory
        batch_size=32,              # Set batch size to 32
        image_size=(640, 640),      # Resize images to 500x500
        shuffle=True,               # Shuffle the dataset
        seed=7,                     # Set seed for reproducibility
        label_mode='categorical'    # Use categorical labels for multi-class classification
    )

    return training_dataset, validation_dataset, test_dataset

# Calling the dataset generation function
training_dataset, validation_dataset, test_dataset = train_val_test_datasets()

# Counting number of batches in each dataset
print(f'Number of training batches: {training_dataset.cardinality()}')
print(f'Number of validation batches: {validation_dataset.cardinality()}')
print(f'Number of test batches: {test_dataset.cardinality()}')

"""# Data pre processing for the Inception v3 Model"""

# Define the preprocess function
def preprocess(image, label):
    image = tf.keras.applications.resnet.preprocess_input(image)
    return image, label

# Apply the preprocessing to the datasets
train_dataset_scaled = training_dataset.map(preprocess)
validation_dataset_scaled = validation_dataset.map(preprocess)
test_dataset_scaled = test_dataset.map(preprocess)

"""# Setup the pretrained model"""

base_model_tf = tf.keras.applications.ResNet50(include_top=False, weights='imagenet', input_shape=(640,640,3), classes=3)

# Unfreeze the top layers of the pretrained ResNet-50 model
base_model_tf.trainable = True

# Freeze all layers except the top layers
for layer in base_model_tf.layers[:-50]:  # Adjust the number of layers to unfreeze as needed
    layer.trainable = False

pt= tf.keras.layers.Input(shape=(640,640,3))
# func = tf.cast(pt,tf.float32) # This line is no longer needed since the input is already float32
x = tf.keras.applications.resnet50.preprocess_input(pt) # Use the preprocess_input function from ResNet50
#x = preprocess_input(pt) #This function used to zero-center each color channel wr

model_resnet = base_model_tf(x,training=False)
model_resnet = tf.keras.layers.GlobalAveragePooling2D()(model_resnet)
model_resnet = tf.keras.layers.Dense(128,activation='relu')(model_resnet)
model_resnet = tf.keras.layers.Dense(64,activation='relu')(model_resnet)
model_resnet = tf.keras.layers.Dense(9,activation='softmax')(model_resnet)

model= tf.keras.Model(inputs=pt,outputs=model_resnet)

model.summary()

# Compile the model with a reduced learning rate
model.compile(
    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-5),  # Lower learning rate
    loss='categorical_crossentropy',
    metrics=['accuracy']
)

"""# Check the model architecture"""

# Checking that the input and output shape of your model are correct
print(f'Input shape: {model.input_shape}')
print(f'Output shape: {model.output_shape}')

# Checking the model summary
model.summary()

# Check that the architecture you used is compatible with the dataset (you can ignore the warnings prompted by using the GPU):
for images, labels in training_dataset.take(1):
	example_batch_images = images
	example_batch_labels = labels

try:
	model.evaluate(example_batch_images, example_batch_labels, verbose=False)
except:
	print("Your model is not compatible with the dataset you defined earlier. Check that the loss function, last layer and label_mode are compatible with one another.")
else:
	predictions = model.predict(example_batch_images, verbose=False)
	print(f"predictions have shape: {predictions.shape}")

"""# Model training"""

# Optimize the datasets for training
SHUFFLE_BUFFER_SIZE = 1000
PREFETCH_BUFFER_SIZE = tf.data.AUTOTUNE

train_dataset_final = (train_dataset_scaled
                       .cache()
                       .shuffle(SHUFFLE_BUFFER_SIZE)
                       .prefetch(PREFETCH_BUFFER_SIZE)
                       )

validation_dataset_final = (validation_dataset_scaled
                            .cache()
                            .prefetch(PREFETCH_BUFFER_SIZE)
                            )

test_dataset_final = (test_dataset_scaled
                      .cache()
                      .prefetch(PREFETCH_BUFFER_SIZE)
                      )

def cosine_annealing(epoch, lr):
    initial_lr = 1e-5  # Adjust the initial learning rate if needed
    return float(initial_lr * (1 + tf.math.cos(epoch * tf.constant(3.14159 / 25))) / 2)

lr_scheduler = tf.keras.callbacks.LearningRateScheduler(cosine_annealing)

# Train the model
history_fine_tuning = model.fit(
    train_dataset_final,
    validation_data=validation_dataset_final,
    epochs=25,  # Increase if needed
    callbacks=[lr_scheduler],  # Include learning rate scheduler
)

"""# Model evaluation"""

def plot_loss_acc(history_fine_tuning):
    '''Plots the training and validation loss and accuracy from a history object'''
    acc = history_fine_tuning.history['accuracy']
    val_acc = history_fine_tuning.history['val_accuracy']
    loss = history_fine_tuning.history['loss']
    val_loss = history_fine_tuning.history['val_loss']

    epochs = range(len(acc))

    fig, ax = plt.subplots(1,2, figsize=(12, 6))
    ax[0].plot(epochs, acc, 'bo', label='Training accuracy')
    ax[0].plot(epochs, val_acc, 'b', label='Validation accuracy')
    ax[0].set_title('Training and validation accuracy')
    ax[0].set_xlabel('epochs')
    ax[0].set_ylabel('accuracy')
    ax[0].legend()

    ax[1].plot(epochs, loss, 'bo', label='Training Loss')
    ax[1].plot(epochs, val_loss, 'b', label='Validation Loss')
    ax[1].set_title('Training and validation loss')
    ax[1].set_xlabel('epochs')
    ax[1].set_ylabel('loss')
    ax[1].legend()

    plt.show()

plot_loss_acc(history_fine_tuning)

# calculating the testung accuracy and the testing loss
test_loss, test_accuracy = model.evaluate(test_dataset)
print(f'Test loss: {test_loss},\nTest accuracy: {test_accuracy}')

"""# Saving the model"""

# Saving model to the google drive
model.save('/content/drive/MyDrive/pepper_mate_models/disease_classification_models/saved_models/resnet_50_2round.keras')
model.save('/content/drive/MyDrive/pepper_mate_models/disease_classification_models/saved_models/resnet_50_2round.h5')

"""# References

- https://www.kaggle.com/code/aryanml007/plant-disease-resnet50
- https://www.tensorflow.org/api_docs/python/tf/keras/applications/resnet/preprocess_input

"""

# prompt: calculate training validation and test accucaries and loss

# Training, validation, and test metrics are already calculated and printed in the provided code.
# Here's a slightly reorganized version to make it more explicit:

# ... (Your existing code) ...

# Model evaluation
# Get training and validation accuracies
acc = history_fine_tuning.history['accuracy']
val_acc = history_fine_tuning.history['val_accuracy']
loss = history_fine_tuning.history['loss']
val_loss = history_fine_tuning.history['val_loss']

# Get number of epochs
epochs = range(len(acc))


# ... (Plotting code) ...

# Calculate and print test accuracy and loss
test_loss, test_accuracy = model.evaluate(test_dataset)
print(f'Test Loss: {test_loss}')
print(f'Test Accuracy: {test_accuracy}')


# Store the metrics in variables for easier access
training_accuracy = acc[-1]  # Accuracy from the last training epoch
validation_accuracy = val_acc[-1] # Accuracy from the last validation epoch
training_loss = loss[-1] # Loss from the last training epoch
validation_loss = val_loss[-1] # Loss from the last validation epoch

print("\nSummary of Metrics:")
print(f"Training Accuracy: {training_accuracy}")
print(f"Validation Accuracy: {validation_accuracy}")
print(f"Training Loss: {training_loss}")
print(f"Validation Loss: {validation_loss}")
print(f"Test Loss: {test_loss}")
print(f"Test Accuracy: {test_accuracy}")

# ... (Rest of your code) ...